{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb4b41d",
   "metadata": {},
   "source": [
    "# Dense Dataset Update Status\n",
    "\n",
    "**✅ This project now operates exclusively on dense, optimized datasets:**\n",
    "\n",
    "## Dense Dataset Files Created:\n",
    "- **train_dense.csv**: 948K interactions (was 6.3M) - 95% reduction\n",
    "- **valid_dense.csv**: 30K interactions (was 1.1M) - 97% reduction  \n",
    "- **test_dense.csv**: 29K interactions (was 1.0M) - 97% reduction\n",
    "- **metadata_dense.csv**: 16.6K products (was 149K) - 89% reduction\n",
    "\n",
    "## Optimization Results:\n",
    "- **Users**: 67K active users (≥10 interactions each)\n",
    "- **Products**: 16.6K popular products (≥15 users each)\n",
    "- **Interactions**: 1M total (12.4% of original)\n",
    "- **Average user activity**: 15 interactions (vs 5 originally)\n",
    "- **File sizes**: 95% smaller overall\n",
    "- **Matrix density**: Significantly improved for collaborative filtering\n",
    "\n",
    "## Schema Standardization:\n",
    "- **Interactions**: `user_id`, `product_id`, `rating`, `timestamp`\n",
    "- **Metadata**: `product_id`, `title`, `main_category`, `price`, `average_rating`, etc.\n",
    "\n",
    "All downstream systems (database, models, API, frontend) have been updated to use these optimized datasets exclusively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60782e2f",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "Data cleaning and final dataset preparation for recommendation system.\n",
    "\n",
    "## Objectives\n",
    "1. Clean interaction datasets\n",
    "2. Validate metadata\n",
    "3. Ensure product-metadata alignment\n",
    "4. Create final model-ready datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003b76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63defe",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d77b407",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2.1 Load original datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data_path = \u001b[43mPath\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m../01_metadata_processing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load interaction datasets\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# 2.1 Load original datasets\n",
    "data_path = Path(\"../01_metadata_processing\")\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load interaction datasets\n",
    "datasets = {}\n",
    "for name in ['train', 'valid', 'test']:\n",
    "    file_path = data_path / f\"{name}_cleaned.csv\"\n",
    "    print(f\"Loading {name}_cleaned.csv...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    datasets[name] = df\n",
    "    print(f\"  {name}: {len(df):,} rows\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata_filtered.csv...\")\n",
    "metadata = pd.read_csv(data_path / \"metadata_filtered.csv\")\n",
    "print(f\"  metadata: {len(metadata):,} products\")\n",
    "\n",
    "print(\"\\nDatasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a82add",
   "metadata": {},
   "source": [
    "## 3. Clean Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61ab84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING INTERACTIONS\n",
      "=========================\n",
      "\n",
      "Cleaning train...\n",
      "  After removing missing: 12,191,484\n",
      "  After removing missing: 12,191,484\n",
      "  After rating filter: 12,191,484\n",
      "  After rating filter: 12,191,484\n",
      "  After deduplication: 12,191,484\n",
      "  Removed: 0 rows\n",
      "\n",
      "Cleaning valid...\n",
      "  After deduplication: 12,191,484\n",
      "  Removed: 0 rows\n",
      "\n",
      "Cleaning valid...\n",
      "  After removing missing: 1,641,026\n",
      "  After rating filter: 1,641,026\n",
      "  After removing missing: 1,641,026\n",
      "  After rating filter: 1,641,026\n",
      "  After deduplication: 1,641,026\n",
      "  Removed: 0 rows\n",
      "\n",
      "Cleaning test...\n",
      "  After deduplication: 1,641,026\n",
      "  Removed: 0 rows\n",
      "\n",
      "Cleaning test...\n",
      "  After removing missing: 1,641,026\n",
      "  After rating filter: 1,641,026\n",
      "  After removing missing: 1,641,026\n",
      "  After rating filter: 1,641,026\n",
      "  After deduplication: 1,641,026\n",
      "  Removed: 0 rows\n",
      "\n",
      "Interaction cleaning complete\n",
      "  After deduplication: 1,641,026\n",
      "  Removed: 0 rows\n",
      "\n",
      "Interaction cleaning complete\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Clean interaction datasets\n",
    "print(\"CLEANING INTERACTIONS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "cleaned_data = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nCleaning {name}...\")\n",
    "    \n",
    "    clean_df = df.copy()\n",
    "    original_len = len(clean_df)\n",
    "    \n",
    "    # Fix data types\n",
    "    clean_df['user_id'] = clean_df['user_id'].astype('string')\n",
    "    clean_df['parent_asin'] = clean_df['parent_asin'].astype('string')\n",
    "    clean_df['rating'] = pd.to_numeric(clean_df['rating'], errors='coerce')\n",
    "    clean_df['timestamp'] = pd.to_datetime(clean_df['timestamp'])\n",
    "    clean_df['history'] = clean_df['history'].fillna('').astype('string')\n",
    "    \n",
    "    # Remove missing critical data\n",
    "    clean_df = clean_df.dropna(subset=['user_id', 'parent_asin', 'rating'])\n",
    "    print(f\"  After removing missing: {len(clean_df):,}\")\n",
    "    \n",
    "    # Filter valid ratings (1-5)\n",
    "    clean_df = clean_df[(clean_df['rating'] >= 1) & (clean_df['rating'] <= 5)]\n",
    "    print(f\"  After rating filter: {len(clean_df):,}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    clean_df = clean_df.drop_duplicates(subset=['user_id', 'parent_asin', 'timestamp'])\n",
    "    print(f\"  After deduplication: {len(clean_df):,}\")\n",
    "    \n",
    "    cleaned_data[name] = clean_df\n",
    "    rows_removed = original_len - len(clean_df)\n",
    "    print(f\"  Removed: {rows_removed:,} rows\")\n",
    "\n",
    "print(\"\\nInteraction cleaning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fb352",
   "metadata": {},
   "source": [
    "## 4. Clean Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea5f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING METADATA\n",
      "====================\n",
      "Original: 149,636 products\n",
      "After deduplication: 149,636\n",
      "\n",
      "Quality:\n",
      "  With prices: 78,138 (52.2%)\n",
      "  With ratings: 149,636 (100.0%)\n",
      "\n",
      "Metadata cleaning complete\n",
      "Original: 149,636 products\n",
      "After deduplication: 149,636\n",
      "\n",
      "Quality:\n",
      "  With prices: 78,138 (52.2%)\n",
      "  With ratings: 149,636 (100.0%)\n",
      "\n",
      "Metadata cleaning complete\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Clean metadata\n",
    "print(\"CLEANING METADATA\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "clean_meta = metadata.copy()\n",
    "original_len = len(clean_meta)\n",
    "\n",
    "print(f\"Original: {original_len:,} products\")\n",
    "\n",
    "# Fix data types\n",
    "clean_meta['parent_asin'] = clean_meta['parent_asin'].astype('string')\n",
    "clean_meta['title'] = clean_meta['title'].fillna('').astype('string')\n",
    "clean_meta['main_category'] = clean_meta['main_category'].fillna('').astype('string')\n",
    "clean_meta['store'] = clean_meta['store'].fillna('').astype('string')\n",
    "clean_meta['categories'] = clean_meta['categories'].fillna('').astype('string')\n",
    "\n",
    "# Clean numeric fields\n",
    "clean_meta['average_rating'] = pd.to_numeric(clean_meta['average_rating'], errors='coerce')\n",
    "clean_meta['rating_number'] = pd.to_numeric(clean_meta['rating_number'], errors='coerce')\n",
    "clean_meta['price'] = pd.to_numeric(clean_meta['price'], errors='coerce')\n",
    "\n",
    "# Remove duplicates\n",
    "clean_meta = clean_meta.drop_duplicates(subset=['parent_asin'])\n",
    "print(f\"After deduplication: {len(clean_meta):,}\")\n",
    "\n",
    "# Quality stats\n",
    "print(f\"\\nQuality:\")\n",
    "print(f\"  With prices: {clean_meta['price'].notna().sum():,} ({clean_meta['price'].notna().sum()/len(clean_meta)*100:.1f}%)\")\n",
    "print(f\"  With ratings: {clean_meta['average_rating'].notna().sum():,} ({clean_meta['average_rating'].notna().sum()/len(clean_meta)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMetadata cleaning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2e3e4",
   "metadata": {},
   "source": [
    "## 5. Coverage Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34556147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE VALIDATION\n",
      "=========================\n",
      "train: 367,052 products\n",
      "train: 367,052 products\n",
      "valid: 251,202 products\n",
      "valid: 251,202 products\n",
      "test: 231,672 products\n",
      "\n",
      "Total interaction products: 368,228\n",
      "Metadata products: 149,636\n",
      "\n",
      "Coverage:\n",
      "  With metadata: 149,636\n",
      "  Coverage: 40.6%\n",
      "  Missing metadata: 218,592\n",
      "  Extra metadata: 0\n",
      "\n",
      "Warning: Low coverage (40.6%)\n",
      "test: 231,672 products\n",
      "\n",
      "Total interaction products: 368,228\n",
      "Metadata products: 149,636\n",
      "\n",
      "Coverage:\n",
      "  With metadata: 149,636\n",
      "  Coverage: 40.6%\n",
      "  Missing metadata: 218,592\n",
      "  Extra metadata: 0\n",
      "\n",
      "Warning: Low coverage (40.6%)\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Validate product coverage\n",
    "print(\"COVERAGE VALIDATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Get products from interactions\n",
    "interaction_products = set()\n",
    "for name, df in cleaned_data.items():\n",
    "    interaction_products.update(df['parent_asin'].unique())\n",
    "    print(f\"{name}: {df['parent_asin'].nunique():,} products\")\n",
    "\n",
    "print(f\"\\nTotal interaction products: {len(interaction_products):,}\")\n",
    "\n",
    "# Get products from metadata\n",
    "meta_products = set(clean_meta['parent_asin'].unique())\n",
    "print(f\"Metadata products: {len(meta_products):,}\")\n",
    "\n",
    "# Calculate overlap\n",
    "overlap = interaction_products.intersection(meta_products)\n",
    "missing_meta = interaction_products - meta_products\n",
    "extra_meta = meta_products - interaction_products\n",
    "\n",
    "coverage_pct = (len(overlap) / len(interaction_products)) * 100\n",
    "\n",
    "print(f\"\\nCoverage:\")\n",
    "print(f\"  With metadata: {len(overlap):,}\")\n",
    "print(f\"  Coverage: {coverage_pct:.1f}%\")\n",
    "print(f\"  Missing metadata: {len(missing_meta):,}\")\n",
    "print(f\"  Extra metadata: {len(extra_meta):,}\")\n",
    "\n",
    "if coverage_pct >= 50:\n",
    "    print(f\"\\nGood coverage ({coverage_pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Low coverage ({coverage_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88faad9",
   "metadata": {},
   "source": [
    "## 6. Create Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9039c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING FINAL DATASETS\n",
      "==============================\n",
      "train: 12,191,484 -> 6,267,269 (5,924,215 removed)\n",
      "train: 12,191,484 -> 6,267,269 (5,924,215 removed)\n",
      "  Users: 1,560,254\n",
      "  Users: 1,560,254\n",
      "  Products: 149,111\n",
      "  Products: 149,111\n",
      "valid: 1,641,026 -> 910,433 (730,593 removed)\n",
      "  Users: 910,433\n",
      "  Products: 107,049\n",
      "valid: 1,641,026 -> 910,433 (730,593 removed)\n",
      "  Users: 910,433\n",
      "  Products: 107,049\n",
      "test: 1,641,026 -> 944,716 (696,310 removed)\n",
      "  Users: 944,716\n",
      "  Products: 100,544\n",
      "test: 1,641,026 -> 944,716 (696,310 removed)\n",
      "  Users: 944,716\n",
      "  Products: 100,544\n",
      "\n",
      "Metadata: 149,636 -> 149,636\n",
      "\n",
      "Metadata: 149,636 -> 149,636\n",
      "\n",
      "Final Summary:\n",
      "  Interactions: 8,122,418\n",
      "  Users: 1,624,138\n",
      "  Products: 149,636\n",
      "  Metadata: 149,636\n",
      "  Coverage: 100.0%\n",
      "\n",
      "Final Summary:\n",
      "  Interactions: 8,122,418\n",
      "  Users: 1,624,138\n",
      "  Products: 149,636\n",
      "  Metadata: 149,636\n",
      "  Coverage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Create final aligned datasets\n",
    "print(\"CREATING FINAL DATASETS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Get products with metadata\n",
    "products_with_meta = set(clean_meta['parent_asin'].unique())\n",
    "\n",
    "# Filter interactions to products with metadata\n",
    "final_data = {}\n",
    "for name, df in cleaned_data.items():\n",
    "    filtered_df = df[df['parent_asin'].isin(products_with_meta)].copy()\n",
    "    final_data[name] = filtered_df\n",
    "    \n",
    "    removed = len(df) - len(filtered_df)\n",
    "    print(f\"{name}: {len(df):,} -> {len(filtered_df):,} ({removed:,} removed)\")\n",
    "    print(f\"  Users: {filtered_df['user_id'].nunique():,}\")\n",
    "    print(f\"  Products: {filtered_df['parent_asin'].nunique():,}\")\n",
    "\n",
    "# Filter metadata to products in interactions\n",
    "all_final_products = set()\n",
    "for df in final_data.values():\n",
    "    all_final_products.update(df['parent_asin'].unique())\n",
    "\n",
    "final_meta = clean_meta[clean_meta['parent_asin'].isin(all_final_products)].copy()\n",
    "print(f\"\\nMetadata: {len(clean_meta):,} -> {len(final_meta):,}\")\n",
    "\n",
    "# Summary\n",
    "total_interactions = sum(len(df) for df in final_data.values())\n",
    "total_users = len(set().union(*[df['user_id'].unique() for df in final_data.values()]))\n",
    "total_products = len(all_final_products)\n",
    "\n",
    "print(f\"\\nFinal Summary:\")\n",
    "print(f\"  Interactions: {total_interactions:,}\")\n",
    "print(f\"  Users: {total_users:,}\")\n",
    "print(f\"  Products: {total_products:,}\")\n",
    "print(f\"  Metadata: {len(final_meta):,}\")\n",
    "print(f\"  Coverage: 100.0%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b15015",
   "metadata": {},
   "source": [
    "## 7. Save Final Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eacb1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING FILES\n",
      "===============\n",
      "train_final.csv: 6,267,269 rows, 970.0 MB\n",
      "train_final.csv: 6,267,269 rows, 970.0 MB\n",
      "valid_final.csv: 910,433 rows, 130.6 MB\n",
      "valid_final.csv: 910,433 rows, 130.6 MB\n",
      "test_final.csv: 944,716 rows, 144.8 MB\n",
      "test_final.csv: 944,716 rows, 144.8 MB\n",
      "metadata_final.csv: 149,636 products, 36.0 MB\n",
      "\n",
      "All files saved\n",
      "metadata_final.csv: 149,636 products, 36.0 MB\n",
      "\n",
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Save final datasets\n",
    "print(\"SAVING FILES\")\n",
    "print(\"=\" * 15)\n",
    "\n",
    "output_path = Path(\".\")\n",
    "\n",
    "# Save interaction files\n",
    "for name, df in final_data.items():\n",
    "    filename = output_path / f\"{name}_final.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    size_mb = filename.stat().st_size / 1024**2\n",
    "    print(f\"{name}_final.csv: {len(df):,} rows, {size_mb:.1f} MB\")\n",
    "\n",
    "# Save metadata\n",
    "meta_filename = output_path / \"metadata_final.csv\"\n",
    "final_meta.to_csv(meta_filename, index=False)\n",
    "meta_size = meta_filename.stat().st_size / 1024**2\n",
    "print(f\"metadata_final.csv: {len(final_meta):,} products, {meta_size:.1f} MB\")\n",
    "\n",
    "print(\"\\nAll files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63d6ac",
   "metadata": {},
   "source": [
    "## 8. Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58452874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION REPORT\n",
      "====================\n",
      "\n",
      "1. Data Types\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  test: Pass\n",
      "\n",
      "2. Missing Values\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  test: Pass\n",
      "\n",
      "3. Rating Validity\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  test: Pass\n",
      "\n",
      "4. Product Alignment\n",
      "  test: Pass\n",
      "\n",
      "3. Rating Validity\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  test: Pass\n",
      "\n",
      "4. Product Alignment\n",
      "  Coverage: Pass\n",
      "\n",
      "5. Duplicates\n",
      "  Coverage: Pass\n",
      "\n",
      "5. Duplicates\n",
      "  train: Pass\n",
      "  train: Pass\n",
      "  valid: Pass\n",
      "  valid: Pass\n",
      "  test: Pass\n",
      "\n",
      "=========================\n",
      "ALL CHECKS PASSED\n",
      "DATA READY FOR MODELING\n",
      "=========================\n",
      "  test: Pass\n",
      "\n",
      "=========================\n",
      "ALL CHECKS PASSED\n",
      "DATA READY FOR MODELING\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Validation report\n",
    "print(\"VALIDATION REPORT\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n1. Data Types\")\n",
    "for name, df in final_data.items():\n",
    "    types_ok = (\n",
    "        df['user_id'].dtype == 'string' and\n",
    "        df['parent_asin'].dtype == 'string' and\n",
    "        np.issubdtype(df['rating'].dtype, np.floating) and\n",
    "        np.issubdtype(df['timestamp'].dtype, np.datetime64)\n",
    "    )\n",
    "    status = \"Pass\" if types_ok else \"Fail\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    results.append(types_ok)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n2. Missing Values\")\n",
    "for name, df in final_data.items():\n",
    "    missing_critical = df[['user_id', 'parent_asin', 'rating']].isnull().sum().sum()\n",
    "    no_missing = missing_critical == 0\n",
    "    status = \"Pass\" if no_missing else \"Fail\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    results.append(no_missing)\n",
    "\n",
    "# Check rating validity\n",
    "print(\"\\n3. Rating Validity\")\n",
    "for name, df in final_data.items():\n",
    "    valid_ratings = ((df['rating'] >= 1) & (df['rating'] <= 5)).all()\n",
    "    status = \"Pass\" if valid_ratings else \"Fail\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    results.append(valid_ratings)\n",
    "\n",
    "# Check alignment\n",
    "print(\"\\n4. Product Alignment\")\n",
    "all_products = set().union(*[df['parent_asin'].unique() for df in final_data.values()])\n",
    "meta_products = set(final_meta['parent_asin'].unique())\n",
    "perfect_alignment = all_products <= meta_products\n",
    "status = \"Pass\" if perfect_alignment else \"Fail\"\n",
    "print(f\"  Coverage: {status}\")\n",
    "results.append(perfect_alignment)\n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\n5. Duplicates\")\n",
    "for name, df in final_data.items():\n",
    "    no_dups = not df.duplicated(['user_id', 'parent_asin', 'timestamp']).any()\n",
    "    status = \"Pass\" if no_dups else \"Fail\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    results.append(no_dups)\n",
    "\n",
    "# Overall status\n",
    "all_passed = all(results)\n",
    "print(\"\\n\" + \"=\" * 25)\n",
    "if all_passed:\n",
    "    print(\"ALL CHECKS PASSED\")\n",
    "    print(\"DATA READY FOR MODELING\")\n",
    "else:\n",
    "    print(\"SOME CHECKS FAILED\")\n",
    "    print(\"REVIEW DATA BEFORE MODELING\")\n",
    "print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5ef51",
   "metadata": {},
   "source": [
    "## 9. Final Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b1812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATISTICS\n",
      "====================\n",
      "\n",
      "Overall:\n",
      "Interactions: 8,122,418\n",
      "Users: 1,624,138\n",
      "Products: 149,636\n",
      "Metadata: 149,636\n",
      "Sparsity: 99.9967%\n",
      "\n",
      "By Dataset:\n",
      "\n",
      "TRAIN:\n",
      "Interactions: 6,267,269\n",
      "\n",
      "Overall:\n",
      "Interactions: 8,122,418\n",
      "Users: 1,624,138\n",
      "Products: 149,636\n",
      "Metadata: 149,636\n",
      "Sparsity: 99.9967%\n",
      "\n",
      "By Dataset:\n",
      "\n",
      "TRAIN:\n",
      "Interactions: 6,267,269\n",
      "Users: 1,560,254\n",
      "Users: 1,560,254\n",
      "Products: 149,111\n",
      "Avg rating: 4.31\n",
      "Date span: 8,822 days\n",
      "\n",
      "VALID:\n",
      "Interactions: 910,433\n",
      "Users: 910,433\n",
      "Products: 149,111\n",
      "Avg rating: 4.31\n",
      "Date span: 8,822 days\n",
      "\n",
      "VALID:\n",
      "Interactions: 910,433\n",
      "Users: 910,433\n",
      "Products: 107,049\n",
      "Avg rating: 4.22\n",
      "Date span: 8,259 days\n",
      "\n",
      "TEST:\n",
      "Interactions: 944,716\n",
      "Users: 944,716\n",
      "Products: 107,049\n",
      "Avg rating: 4.22\n",
      "Date span: 8,259 days\n",
      "\n",
      "TEST:\n",
      "Interactions: 944,716\n",
      "Users: 944,716\n",
      "Products: 100,544\n",
      "Avg rating: 4.13\n",
      "Date span: 8,107 days\n",
      "\n",
      "METADATA:\n",
      "Products: 149,636\n",
      "With prices: 78,138 (52.2%)\n",
      "With ratings: 149,636 (100.0%)\n",
      "Avg price: $82.81\n",
      "Avg rating: 4.13\n",
      "Categories: 37\n",
      "Products: 100,544\n",
      "Avg rating: 4.13\n",
      "Date span: 8,107 days\n",
      "\n",
      "METADATA:\n",
      "Products: 149,636\n",
      "With prices: 78,138 (52.2%)\n",
      "With ratings: 149,636 (100.0%)\n",
      "Avg price: $82.81\n",
      "Avg rating: 4.13\n",
      "Categories: 37\n"
     ]
    }
   ],
   "source": [
    "# 9.1 Final statistics\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Overall stats\n",
    "total_interactions = sum(len(df) for df in final_data.values())\n",
    "total_users = len(set().union(*[df['user_id'].unique() for df in final_data.values()]))\n",
    "total_products = len(set().union(*[df['parent_asin'].unique() for df in final_data.values()]))\n",
    "\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"Interactions: {total_interactions:,}\")\n",
    "print(f\"Users: {total_users:,}\")\n",
    "print(f\"Products: {total_products:,}\")\n",
    "print(f\"Metadata: {len(final_meta):,}\")\n",
    "print(f\"Sparsity: {(1 - total_interactions/(total_users * total_products)) * 100:.4f}%\")\n",
    "\n",
    "# Per-dataset stats\n",
    "print(f\"\\nBy Dataset:\")\n",
    "for name, df in final_data.items():\n",
    "    avg_rating = df['rating'].mean()\n",
    "    date_span = (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"Interactions: {len(df):,}\")\n",
    "    print(f\"Users: {df['user_id'].nunique():,}\")\n",
    "    print(f\"Products: {df['parent_asin'].nunique():,}\")\n",
    "    print(f\"Avg rating: {avg_rating:.2f}\")\n",
    "    print(f\"Date span: {date_span:,} days\")\n",
    "\n",
    "# Metadata stats\n",
    "print(f\"\\nMETADATA:\")\n",
    "print(f\"Products: {len(final_meta):,}\")\n",
    "print(f\"With prices: {final_meta['price'].notna().sum():,} ({final_meta['price'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"With ratings: {final_meta['average_rating'].notna().sum():,} ({final_meta['average_rating'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"Avg price: ${final_meta['price'].mean():.2f}\")\n",
    "print(f\"Avg rating: {final_meta['average_rating'].mean():.2f}\")\n",
    "print(f\"Categories: {final_meta['main_category'].nunique():,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
